{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# pip install pytorch_pretrained_bert\n",
    "import pytorch_pretrained_bert\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# local path of downloaded files\n",
    "model_path = '/home/model/'\n",
    "vocab_path = '/home/vocab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set model path\n",
    "\n",
    "pytorch_pretrained_bert.modeling.PRETRAINED_MODEL_ARCHIVE_MAP = {\n",
    "   'bert-base-uncased': model_path+'bert-base-uncased.tar.gz',\n",
    " 'bert-large-uncased': model_path+'bert-large-uncased.tar.gz',\n",
    " 'bert-base-cased': model_path+'bert-base-cased.tar.gz',\n",
    " 'bert-large-cased': model_path+'bert-large-cased.tar.gz',\n",
    " 'bert-base-multilingual-uncased': model_path+'bert-base-multilingual-uncased.tar.gz',\n",
    " 'bert-base-multilingual-cased': model_path+'bert-base-multilingual-cased.tar.gz',\n",
    " 'bert-base-chinese': model_path+'bert-base-chinese.tar.gz'    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pytorch_pretrained_bert.tokenization.PRETRAINED_VOCAB_ARCHIVE_MAP = {\n",
    "    'bert-base-uncased': vocab_path+\"bert-base-uncased-vocab.txt\",\n",
    "    'bert-large-uncased': vocab_path+\"bert-large-uncased-vocab.txt\",\n",
    "    'bert-base-cased': vocab_path+\"bert-base-cased-vocab.txt\",\n",
    "    'bert-large-cased': vocab_path+\"bert-large-cased-vocab.txt\",\n",
    "    'bert-base-multilingual-uncased': vocab_path+\"bert-base-multilingual-uncased-vocab.txt\",\n",
    "    'bert-base-multilingual-cased': vocab_path+\"bert-base-multilingual-cased-vocab.txt\",\n",
    "    'bert-base-chinese': vocab_path+\"bert-base-chinese-vocab.txt\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text preprocessing assuming that the input is raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_feature(sentence,seq_length=512):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    tokenized_text = tokenizer.tokenize(sentence)\n",
    "    tokens = ['[CLS]'] + tokenized_text + ['[SEP]']\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    input_mask = [1] * len(input_ids)\n",
    "            # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "    \n",
    "    return input_ids, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentences_to_features(sentence_batch):\n",
    "    \"\"\"Use this as the batch input to the forward method of  Akimoto_BERT\"\"\"\n",
    "    input_ids_, input_mask_ = [], []\n",
    "    for sentence in sentence_batch:\n",
    "        input_ids, input_mask = sentence_to_feature(sentence)\n",
    "        input_ids_.append(input_ids)\n",
    "        input_mask_.append(input_mask)\n",
    "    all_input_ids = torch.tensor([f for f in input_ids_], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f for f in input_mask_], dtype=torch.long)\n",
    "    return all_input_ids, all_input_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build model here based on bert embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Akimoto_BERT(nn.Module):\n",
    "    def __init__(self, data_parallel=True):\n",
    "        bert = BertModel.from_pretrained(\"bert-base-uncased\").to(device=torch.device(\"cuda\"))\n",
    "        if data_parallel:\n",
    "            self.bert = torch.nn.DataParallel(bert)\n",
    "    else:\n",
    "        self.bert = bert\n",
    "    # other init from akimoto model\n",
    "    # droput, log_softmax...\n",
    "    \n",
    "    def forward(self,bert_batch):\n",
    "        bert_ids, bert_mask = bert_batch\n",
    "           \n",
    "        segment_ids = torch.zeros_like(bert_mask)\n",
    "        bert_last_layer = self.bert(bert_ids, bert_mask, segment_ids)[0][-1] # this is the bert embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
